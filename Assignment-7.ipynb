{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNT3tRrlKOtt9I58x8/az2E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaiChandraDevulapally/2303a51l14_27/blob/main/Assignment-7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdB1U5JBM_ci",
        "outputId": "2ae31a2c-d26a-48df-83a8-27f5c7231a68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 375ms/step - accuracy: 0.1750 - loss: 2.8807 - val_accuracy: 0.5000 - val_loss: 2.8402\n",
            "Input: hello\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 286ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "Translated: comment je je je je\n",
            "Input: thank you\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "Translated: comment je je je je\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, GRU, Dense\n",
        "\n",
        "# Example small dataset: English to French sentence pairs\n",
        "data = [\n",
        "    (\"hello\", \"bonjour\"),\n",
        "    (\"how are you\", \"comment ça va\"),\n",
        "    (\"I am fine\", \"je vais bien\"),\n",
        "    (\"what is your name\", \"comment tu t'appelles\"),\n",
        "    (\"my name is\", \"je m'appelle\"),\n",
        "    (\"thank you\", \"merci\"),\n",
        "    (\"goodbye\", \"au revoir\")\n",
        "]\n",
        "\n",
        "# (a) Data Preprocessing\n",
        "english_sentences = [pair[0] for pair in data]\n",
        "french_sentences = ['<start> ' + pair[1] + ' <end>' for pair in data]  # Adding start/end tokens for decoder\n",
        "\n",
        "# Tokenize English and French sentences\n",
        "english_tokenizer = Tokenizer()\n",
        "english_tokenizer.fit_on_texts(english_sentences)\n",
        "english_vocab_size = len(english_tokenizer.word_index) + 1  # plus 1 for padding\n",
        "\n",
        "french_tokenizer = Tokenizer()\n",
        "french_tokenizer.fit_on_texts(french_sentences)\n",
        "french_tokenizer.word_index['<start>'] = len(french_tokenizer.word_index) + 1\n",
        "french_tokenizer.word_index['<end>'] = len(french_tokenizer.word_index) + 1\n",
        "french_vocab_size = len(french_tokenizer.word_index) + 1  # plus 1 for padding\n",
        "\n",
        "# Convert sentences to sequences\n",
        "encoder_input_sequences = english_tokenizer.texts_to_sequences(english_sentences)\n",
        "decoder_input_sequences = french_tokenizer.texts_to_sequences(french_sentences)\n",
        "\n",
        "# Padding sequences\n",
        "max_encoder_seq_length = max(len(seq) for seq in encoder_input_sequences)\n",
        "max_decoder_seq_length = max(len(seq) for seq in decoder_input_sequences)\n",
        "\n",
        "encoder_input_sequences = pad_sequences(encoder_input_sequences, maxlen=max_encoder_seq_length, padding='post')\n",
        "decoder_input_sequences = pad_sequences(decoder_input_sequences, maxlen=max_decoder_seq_length, padding='post')\n",
        "\n",
        "# Prepare decoder output sequences (shifted by one position)\n",
        "decoder_output_sequences = np.zeros_like(decoder_input_sequences)\n",
        "decoder_output_sequences[:, :-1] = decoder_input_sequences[:, 1:]\n",
        "\n",
        "# (b) Build Seq2Seq Model\n",
        "encoder_inputs = Input(shape=(max_encoder_seq_length,))\n",
        "encoder_embedding = Embedding(english_vocab_size, 64)(encoder_inputs)\n",
        "encoder_gru = GRU(128, return_state=True)\n",
        "encoder_outputs, encoder_state = encoder_gru(encoder_embedding)\n",
        "\n",
        "decoder_inputs = Input(shape=(max_decoder_seq_length,))\n",
        "decoder_embedding = Embedding(french_vocab_size, 64)(decoder_inputs)\n",
        "decoder_gru = GRU(128, return_sequences=True)(decoder_embedding, initial_state=encoder_state)\n",
        "decoder_dense = Dense(french_vocab_size, activation='softmax')\n",
        "\n",
        "decoder_outputs = decoder_dense(decoder_gru)\n",
        "\n",
        "# Define the model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# (c) Train the Model\n",
        "model.fit([encoder_input_sequences, decoder_input_sequences],\n",
        "          np.expand_dims(decoder_output_sequences, -1),\n",
        "          batch_size=2, validation_split=0.2)\n",
        "\n",
        "# (d) Inference Setup for Translation\n",
        "# Encoder model for extracting states\n",
        "encoder_model = Model(encoder_inputs, encoder_state)\n",
        "\n",
        "# Decoder model for generating output\n",
        "decoder_state_input = Input(shape=(128,))\n",
        "decoder_gru_inf = GRU(128, return_sequences=True, return_state=True)\n",
        "decoder_outputs, decoder_state = decoder_gru_inf(decoder_embedding, initial_state=decoder_state_input)\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "decoder_model = Model([decoder_inputs, decoder_state_input], [decoder_outputs, decoder_state])\n",
        "\n",
        "# (e) Translate New Sentences\n",
        "def translate(input_sentence):\n",
        "    # Encode the input sentence\n",
        "    input_seq = english_tokenizer.texts_to_sequences([input_sentence])\n",
        "    input_seq = pad_sequences(input_seq, maxlen=max_encoder_seq_length, padding='post')\n",
        "\n",
        "    # Get the initial state of the encoder\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Start with the \"start token\" for the French sentence\n",
        "    target_seq = np.array([[french_tokenizer.word_index['<start>']]])\n",
        "\n",
        "    # Generate the output sequence one token at a time\n",
        "    translated_sentence = ''\n",
        "    for _ in range(max_decoder_seq_length):\n",
        "        output_tokens, states_value = decoder_model.predict([target_seq, states_value])\n",
        "        # Get the token with the highest probability\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "\n",
        "        # Handle out-of-vocabulary words\n",
        "        if sampled_token_index in french_tokenizer.index_word:\n",
        "            sampled_word = french_tokenizer.index_word[sampled_token_index]\n",
        "        else:\n",
        "            sampled_word = \"<unk>\" # or any placeholder for unknown words\n",
        "\n",
        "        # Stop if we reach the \"end token\"\n",
        "        if sampled_word == '<end>':\n",
        "            break\n",
        "\n",
        "        translated_sentence += ' ' + sampled_word\n",
        "        target_seq = np.array([[sampled_token_index]])\n",
        "\n",
        "    return translated_sentence.strip()\n",
        "\n",
        "# (f) Example: Translate New Sentences\n",
        "input_sentence = \"hello\"\n",
        "print(\"Input:\", input_sentence)\n",
        "print(\"Translated:\", translate(input_sentence))\n",
        "\n",
        "# Example: Translate another sentence\n",
        "input_sentence = \"thank you\"\n",
        "print(\"Input:\", input_sentence)\n",
        "print(\"Translated:\", translate(input_sentence))\n",
        "\n",
        "# (g) Experimenting and Improving the Model\n",
        "# - Experiment with a larger dataset like the Tatoeba dataset for better generalization.\n",
        "# - Try hyperparameter tuning: e.g., batch size, learning rate, number of GRU units, etc.\n",
        "# - Consider adding an attention mechanism or using Transformer models for better performance."
      ]
    }
  ]
}