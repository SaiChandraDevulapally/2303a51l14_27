{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7qXF0zCsGsZJu/MkBfMzZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Adithyahh/2203A51804-NLP/blob/main/Assignment%208.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAnOFmGVpV8l",
        "outputId": "0c001acb-3999-4bfa-9c9e-9e7c0d0876ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with 20 epochs\n",
            "Epoch 1/20, Loss: 1.1686850786209106\n",
            "Epoch 2/20, Loss: 0.4779810309410095\n",
            "Epoch 3/20, Loss: 0.15723954141139984\n",
            "Epoch 4/20, Loss: 0.08946692943572998\n",
            "Epoch 5/20, Loss: 0.06836246699094772\n",
            "Epoch 6/20, Loss: 0.06429968774318695\n",
            "Epoch 7/20, Loss: 0.07478256523609161\n",
            "Epoch 8/20, Loss: 0.0884733721613884\n",
            "Epoch 9/20, Loss: 0.05638786777853966\n",
            "Epoch 10/20, Loss: 0.027910945937037468\n",
            "Epoch 11/20, Loss: 0.039185889065265656\n",
            "Epoch 12/20, Loss: 0.09135650843381882\n",
            "Epoch 13/20, Loss: 0.03171371668577194\n",
            "Epoch 14/20, Loss: 0.20846140384674072\n",
            "Epoch 15/20, Loss: 0.0012732901377603412\n",
            "Epoch 16/20, Loss: 0.04455846548080444\n",
            "Epoch 17/20, Loss: 0.009406703524291515\n",
            "Epoch 18/20, Loss: 0.002999716904014349\n",
            "Epoch 19/20, Loss: 0.004810686223208904\n",
            "Epoch 20/20, Loss: 0.1789366900920868\n",
            "Training with 60 epochs\n",
            "Epoch 1/60, Loss: 0.0003511669347062707\n",
            "Epoch 2/60, Loss: 0.004236217588186264\n",
            "Epoch 3/60, Loss: 0.0003005146572832018\n",
            "Epoch 4/60, Loss: 0.006520267575979233\n",
            "Epoch 5/60, Loss: 0.000993371824733913\n",
            "Epoch 6/60, Loss: 0.04448302835226059\n",
            "Epoch 7/60, Loss: 0.006788219790905714\n",
            "Epoch 8/60, Loss: 0.000729728548321873\n",
            "Epoch 9/60, Loss: 0.07977674156427383\n",
            "Epoch 10/60, Loss: 0.028765613213181496\n",
            "Epoch 11/60, Loss: 0.07056527584791183\n",
            "Epoch 12/60, Loss: 0.0001291896915063262\n",
            "Epoch 13/60, Loss: 0.00011450374586274847\n",
            "Epoch 14/60, Loss: 0.17793604731559753\n",
            "Epoch 15/60, Loss: 0.16661211848258972\n",
            "Epoch 16/60, Loss: 0.00434458814561367\n",
            "Epoch 17/60, Loss: 0.0008057892555370927\n",
            "Epoch 18/60, Loss: 0.0014944820431992412\n",
            "Epoch 19/60, Loss: 0.06931889057159424\n",
            "Epoch 20/60, Loss: 0.00032808963442221284\n",
            "Epoch 21/60, Loss: 0.07325678318738937\n",
            "Epoch 22/60, Loss: 0.17186369001865387\n",
            "Epoch 23/60, Loss: 0.005682709161192179\n",
            "Epoch 24/60, Loss: 0.16001437604427338\n",
            "Epoch 25/60, Loss: 0.1605333536863327\n",
            "Epoch 26/60, Loss: 0.004210254643112421\n",
            "Epoch 27/60, Loss: 0.0003901419695466757\n",
            "Epoch 28/60, Loss: 0.0017950698966160417\n",
            "Epoch 29/60, Loss: 0.04771433398127556\n",
            "Epoch 30/60, Loss: 0.0001015909219859168\n",
            "Epoch 31/60, Loss: 0.07223281264305115\n",
            "Epoch 32/60, Loss: 0.007832194678485394\n",
            "Epoch 33/60, Loss: 0.00012203190271975473\n",
            "Epoch 34/60, Loss: 0.00014713132986798882\n",
            "Epoch 35/60, Loss: 0.016634907573461533\n",
            "Epoch 36/60, Loss: 0.07136917859315872\n",
            "Epoch 37/60, Loss: 0.22523367404937744\n",
            "Epoch 38/60, Loss: 0.0006699851946905255\n",
            "Epoch 39/60, Loss: 4.9086258513852954e-05\n",
            "Epoch 40/60, Loss: 0.009067104198038578\n",
            "Epoch 41/60, Loss: 0.00037811059155501425\n",
            "Epoch 42/60, Loss: 6.049517833162099e-05\n",
            "Epoch 43/60, Loss: 0.00014776397438254207\n",
            "Epoch 44/60, Loss: 0.00016362783208023757\n",
            "Epoch 45/60, Loss: 0.023154020309448242\n",
            "Epoch 46/60, Loss: 2.07362409128109e-05\n",
            "Epoch 47/60, Loss: 0.005920480936765671\n",
            "Epoch 48/60, Loss: 0.0001558352669235319\n",
            "Epoch 49/60, Loss: 8.417168282903731e-05\n",
            "Epoch 50/60, Loss: 0.0007613234920427203\n",
            "Epoch 51/60, Loss: 6.83979014866054e-05\n",
            "Epoch 52/60, Loss: 0.0029662041924893856\n",
            "Epoch 53/60, Loss: 0.01658889278769493\n",
            "Epoch 54/60, Loss: 0.00025124731473624706\n",
            "Epoch 55/60, Loss: 0.00020016955386381596\n",
            "Epoch 56/60, Loss: 0.13846132159233093\n",
            "Epoch 57/60, Loss: 0.00015907919441815466\n",
            "Epoch 58/60, Loss: 0.11980140209197998\n",
            "Epoch 59/60, Loss: 0.00011284685751888901\n",
            "Epoch 60/60, Loss: 0.0004501081130001694\n",
            "Training with 70 epochs\n",
            "Epoch 1/70, Loss: 6.053798642824404e-05\n",
            "Epoch 2/70, Loss: 0.00020833774760831147\n",
            "Epoch 3/70, Loss: 0.16957321763038635\n",
            "Epoch 4/70, Loss: 0.00042947681504301727\n",
            "Epoch 5/70, Loss: 0.0001819809986045584\n",
            "Epoch 6/70, Loss: 0.0003349672188051045\n",
            "Epoch 7/70, Loss: 9.259609214495867e-05\n",
            "Epoch 8/70, Loss: 0.00011480214016046375\n",
            "Epoch 9/70, Loss: 0.0007209805189631879\n",
            "Epoch 10/70, Loss: 0.0025850075762718916\n",
            "Epoch 11/70, Loss: 0.00043758226092904806\n",
            "Epoch 12/70, Loss: 7.39459865144454e-05\n",
            "Epoch 13/70, Loss: 0.030998514965176582\n",
            "Epoch 14/70, Loss: 0.1265796422958374\n",
            "Epoch 15/70, Loss: 0.00021776986250188202\n",
            "Epoch 16/70, Loss: 0.0003498464357107878\n",
            "Epoch 17/70, Loss: 0.00044145152787677944\n",
            "Epoch 18/70, Loss: 0.00011217718565603718\n",
            "Epoch 19/70, Loss: 0.000579428393393755\n",
            "Epoch 20/70, Loss: 0.006543237250298262\n",
            "Epoch 21/70, Loss: 0.0002084622683469206\n",
            "Epoch 22/70, Loss: 9.130109538091347e-05\n",
            "Epoch 23/70, Loss: 0.00045422976836562157\n",
            "Epoch 24/70, Loss: 4.941344013786875e-05\n",
            "Epoch 25/70, Loss: 0.007612896151840687\n",
            "Epoch 26/70, Loss: 0.00014883436961099505\n",
            "Epoch 27/70, Loss: 0.00010451826528878883\n",
            "Epoch 28/70, Loss: 0.00010511562140891328\n",
            "Epoch 29/70, Loss: 0.000439437513705343\n",
            "Epoch 30/70, Loss: 0.05258651077747345\n",
            "Epoch 31/70, Loss: 9.969640814233571e-05\n",
            "Epoch 32/70, Loss: 0.00019181164680048823\n",
            "Epoch 33/70, Loss: 8.161534060491249e-05\n",
            "Epoch 34/70, Loss: 0.0001230817724717781\n",
            "Epoch 35/70, Loss: 0.0005254246643744409\n",
            "Epoch 36/70, Loss: 0.00027135363779962063\n",
            "Epoch 37/70, Loss: 0.038718752562999725\n",
            "Epoch 38/70, Loss: 1.1434098269091919e-05\n",
            "Epoch 39/70, Loss: 0.00011883909610332921\n",
            "Epoch 40/70, Loss: 0.00023795374727342278\n",
            "Epoch 41/70, Loss: 0.0004563931142911315\n",
            "Epoch 42/70, Loss: 0.003187010530382395\n",
            "Epoch 43/70, Loss: 0.016577335074543953\n",
            "Epoch 44/70, Loss: 9.576871525496244e-05\n",
            "Epoch 45/70, Loss: 0.12469574809074402\n",
            "Epoch 46/70, Loss: 0.0010754273971542716\n",
            "Epoch 47/70, Loss: 0.014790331944823265\n",
            "Epoch 48/70, Loss: 0.0007968957652337849\n",
            "Epoch 49/70, Loss: 0.007026068400591612\n",
            "Epoch 50/70, Loss: 0.006615299265831709\n",
            "Epoch 51/70, Loss: 0.00015306854038499296\n",
            "Epoch 52/70, Loss: 0.02025626040995121\n",
            "Epoch 53/70, Loss: 9.977097943192348e-05\n",
            "Epoch 54/70, Loss: 0.0001342729083262384\n",
            "Epoch 55/70, Loss: 0.0002452499174978584\n",
            "Epoch 56/70, Loss: 0.0003575300215743482\n",
            "Epoch 57/70, Loss: 0.001648884848691523\n",
            "Epoch 58/70, Loss: 0.00014015258057042956\n",
            "Epoch 59/70, Loss: 0.0011645244667306542\n",
            "Epoch 60/70, Loss: 7.813082629581913e-05\n",
            "Epoch 61/70, Loss: 0.0004443035868462175\n",
            "Epoch 62/70, Loss: 0.00033914021332748234\n",
            "Epoch 63/70, Loss: 5.180851076147519e-05\n",
            "Epoch 64/70, Loss: 0.0019998308271169662\n",
            "Epoch 65/70, Loss: 9.596857125870883e-05\n",
            "Epoch 66/70, Loss: 2.477557245583739e-05\n",
            "Epoch 67/70, Loss: 0.0031914105638861656\n",
            "Epoch 68/70, Loss: 8.072914351942018e-05\n",
            "Epoch 69/70, Loss: 0.11418508738279343\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 70/70, Loss: 4.95052918267902e-05\n",
            "Generated Text: Once upon a time, there was a little girl named Red Riding Hood. She loved to visit her grandmother, who lived in the woods.\n",
            "One day, her mother asked her to take a basket of goodies to her grandma. On her way through\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "# Set up device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Example data (Use a larger dataset for meaningful training)\n",
        "text = \"\"\"\n",
        "Once upon a time, there was a little girl named Red Riding Hood. She loved to visit her grandmother, who lived in the woods.\n",
        "One day, her mother asked her to take a basket of goodies to her grandmother. On her way through the woods, she met a big bad wolf who wanted to eat her.\n",
        "\"\"\"\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Set pad token to eos_token to avoid NoneType error\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.to(device)\n",
        "\n",
        "# Prepare dataset without padding here, as padding will be handled in the collate function\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text, tokenizer, max_length=50):\n",
        "        self.tokens = tokenizer(text, return_tensors=\"pt\", truncation=True)[\"input_ids\"][0]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokens) - 50  # Number of training steps\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.tokens[idx:idx+50]\n",
        "\n",
        "# Custom collate function for dynamic padding\n",
        "def collate_fn(batch):\n",
        "    max_length = max([len(x) for x in batch])\n",
        "    padded_batch = [torch.cat([x, torch.full((max_length - len(x),), tokenizer.pad_token_id)]) for x in batch]\n",
        "    return torch.stack(padded_batch)\n",
        "\n",
        "dataset = TextDataset(text, tokenizer)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# Training function\n",
        "def train_model(epochs):\n",
        "    model.train()\n",
        "    optimizer = AdamW(model.parameters(), lr=3e-5)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for batch in dataloader:\n",
        "            inputs = batch.to(device)\n",
        "            labels = inputs.clone()\n",
        "            outputs = model(inputs, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "# Train the model with different epochs\n",
        "for epochs in [20, 60, 70]:\n",
        "    print(f\"Training with {epochs} epochs\")\n",
        "    train_model(epochs)\n",
        "\n",
        "# Text generation function\n",
        "def generate_text(seed_text, max_length=50):\n",
        "    model.eval()\n",
        "    input_ids = tokenizer.encode(seed_text, return_tensors=\"pt\").to(device)\n",
        "    generated_ids = model.generate(input_ids, max_length=max_length, num_return_sequences=1, no_repeat_ngram_size=2)\n",
        "    return tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Example of generating new text with seed text\n",
        "seed_text = \"Once upon a time\"\n",
        "generated_text = generate_text(seed_text)\n",
        "print(\"Generated Text:\", generated_text)\n"
      ]
    }
  ]
}